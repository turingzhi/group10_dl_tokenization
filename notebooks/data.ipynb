{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WikiText-2 dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4482891344b169288e85fd5e617cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8415fb1ff9e8471986728aa3abe47215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dbc42a73fd450b82a4da06e7287155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Train examples: 36718\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import re\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "\n",
    "# --- Configuration ---\n",
    "VOCAB_SIZE_SUBWORD = 32000  # For BPE and Unigram\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 3\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# 1. Load the WikiText-2 Dataset\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "# Using the 'raw' version which is better for training custom tokenizers\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Pre-cleaning function to remove document titles and extra newlines\n",
    "def clean_text(examples):\n",
    "    # Remove lines starting with '= Title =' and strip excessive whitespace\n",
    "    text = examples[\"text\"]\n",
    "    text = re.sub(r'= .*? =', '', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    examples[\"text\"] = text.strip()\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(clean_text, batched=False)\n",
    "\n",
    "train_data = dataset[\"train\"][\"text\"]\n",
    "valid_data = dataset[\"validation\"][\"text\"]\n",
    "test_data = dataset[\"test\"][\"text\"]\n",
    "\n",
    "print(f\"Data loaded. Train examples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Word Tokenizer...\n",
      "Creating Byte Tokenizer...\n",
      "Training BPE Tokenizer with vocab size 32000...\n",
      "\n",
      "\n",
      "\n",
      "Training Unigram Tokenizer with vocab size 32000...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_get_tokenizers(train_data, vocab_size_subword):\n",
    "    # Special tokens required for all models\n",
    "    special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "    \n",
    "    # --- 1. Word Tokenizer (Simple Split + Fixed Vocab) ---\n",
    "    print(\"\\nTraining Word Tokenizer...\")\n",
    "    \n",
    "    word_counts = {}\n",
    "    for text in train_data:\n",
    "        for word in text.split():\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            \n",
    "    # Keep only the top 10000 words + special tokens\n",
    "    word_vocab = special_tokens + sorted(word_counts, key=word_counts.get, reverse=True)[:10000]\n",
    "    \n",
    "    # Create a mapping (vocab_size will be len(word_vocab))\n",
    "    word_to_id = {word: i for i, word in enumerate(word_vocab)}\n",
    "    \n",
    "    class WordTokenizer:\n",
    "        def __init__(self, word_to_id, unk_id=0):\n",
    "            self.word_to_id = word_to_id\n",
    "            self.unk_id = unk_id\n",
    "            self.pad_token_id = self.word_to_id.get(\"<pad>\")\n",
    "            self.vocab_size = len(word_to_id)\n",
    "        \n",
    "        def encode(self, text):\n",
    "            # Simple whitespace split and lookup\n",
    "            return [self.word_to_id.get(word, self.unk_id) for word in text.split()]\n",
    "        \n",
    "    word_tokenizer = WordTokenizer(word_to_id)\n",
    "    \n",
    "    \n",
    "    # --- 2. Byte Tokenizer (Character Level) ---\n",
    "    print(\"Creating Byte Tokenizer...\")\n",
    "    \n",
    "    # Vocabulary size is 256 (for standard ASCII/bytes) + special tokens\n",
    "    byte_vocab_size = 256 + len(special_tokens)\n",
    "    byte_to_id = {chr(i): i + len(special_tokens) for i in range(256)}\n",
    "    # Add special tokens mapping manually\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        byte_to_id[token] = i\n",
    "\n",
    "    class ByteTokenizer:\n",
    "        def __init__(self, byte_to_id, unk_id=0):\n",
    "            self.byte_to_id = byte_to_id\n",
    "            self.unk_id = unk_id\n",
    "            self.pad_token_id = self.byte_to_id.get(\"<pad>\")\n",
    "            self.vocab_size = len(byte_to_id)\n",
    "\n",
    "        def encode(self, text):\n",
    "            # Map characters to their ID. Uses ord() for simple chars.\n",
    "            return [self.byte_to_id.get(c, self.unk_id) for c in text]\n",
    "\n",
    "    byte_tokenizer = ByteTokenizer(byte_to_id)\n",
    "\n",
    "\n",
    "    # --- 3. BPE Tokenizer (Trained) ---\n",
    "    print(f\"Training BPE Tokenizer with vocab size {vocab_size_subword}...\")\n",
    "    bpe_tokenizer = Tokenizer(models.BPE())\n",
    "    bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    bpe_trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size_subword,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "    bpe_tokenizer.train_from_iterator(train_data, trainer=bpe_trainer)\n",
    "    # Post-processor to handle special tokens for the model\n",
    "    bpe_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        pair=\"<bos> $A <eos> $B\",\n",
    "        special_tokens=[(\"<bos>\", bpe_tokenizer.token_to_id(\"<bos>\")),\n",
    "                        (\"<eos>\", bpe_tokenizer.token_to_id(\"<eos>\"))]\n",
    "    )\n",
    "    bpe_tokenizer.pad_token_id = bpe_tokenizer.token_to_id(\"<pad>\")\n",
    "    bpe_tokenizer.vocab_size = bpe_tokenizer.get_vocab_size()\n",
    "\n",
    "\n",
    "    # --- 4. Unigram Tokenizer (Trained) ---\n",
    "    print(f\"Training Unigram Tokenizer with vocab size {vocab_size_subword}...\")\n",
    "    unigram_tokenizer = Tokenizer(models.Unigram())\n",
    "    unigram_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    unigram_trainer = trainers.UnigramTrainer(\n",
    "        vocab_size=vocab_size_subword,\n",
    "        special_tokens=special_tokens,\n",
    "        shrinking_factor=0.75\n",
    "    )\n",
    "    unigram_tokenizer.train_from_iterator(train_data, trainer=unigram_trainer)\n",
    "    # Post-processor to handle special tokens for the model\n",
    "    unigram_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        pair=\"<bos> $A <eos> $B\",\n",
    "        special_tokens=[(\"<bos>\", unigram_tokenizer.token_to_id(\"<bos>\")),\n",
    "                        (\"<eos>\", unigram_tokenizer.token_to_id(\"<eos>\"))]\n",
    "    )\n",
    "    unigram_tokenizer.pad_token_id = unigram_tokenizer.token_to_id(\"<pad>\")\n",
    "    unigram_tokenizer.vocab_size = unigram_tokenizer.get_vocab_size()\n",
    "\n",
    "    \n",
    "    # Store tokenizers and their vocab sizes\n",
    "    tokenizers = {\n",
    "        \"word\": word_tokenizer,\n",
    "        \"bpe\": bpe_tokenizer,\n",
    "        \"byte\": byte_tokenizer,\n",
    "        \"unigram\": unigram_tokenizer\n",
    "    }\n",
    "    \n",
    "    return tokenizers\n",
    "\n",
    "tokenizers = train_and_get_tokenizers(train_data, VOCAB_SIZE_SUBWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # --- CLEANUP: Directly use the standardized attribute ---\n",
    "        self.pad_id = tokenizer.pad_token_id \n",
    "        # --------------------------------------------------------\n",
    "        self.all_ids = self._tokenize_and_chunk(data)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Handle different tokenizer types\n",
    "        if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
    "            return tokenizer.pad_token_id\n",
    "        elif hasattr(tokenizer, 'word_to_id'): # WordTokenizer\n",
    "            return tokenizer.word_to_id.get(\"<pad>\")\n",
    "        elif hasattr(tokenizer, 'byte_to_id'): # ByteTokenizer\n",
    "            return tokenizer.byte_to_id.get(\"<pad>\")\n",
    "        return 1 # Fallback, assumes <pad> is at index 1\n",
    "\n",
    "    def _tokenize_and_chunk(self, data):\n",
    "        print(f\"Tokenizing and chunking data (PAD ID: {self.pad_id}, Vocab Size: {self.tokenizer.vocab_size})...\")\n",
    "        full_token_list = []\n",
    "        for text in data:\n",
    "            if text.strip():\n",
    "                # Get the IDs list from the tokenizer\n",
    "                if hasattr(self.tokenizer, 'encode'): # Custom Word/Byte\n",
    "                    ids = self.tokenizer.encode(text)\n",
    "                else: # HF tokenizers\n",
    "                    ids = self.tokenizer.encode(text).ids\n",
    "                full_token_list.extend(ids)\n",
    "\n",
    "        # Chunk the large list of IDs into sequences of max_seq_len\n",
    "        all_ids = []\n",
    "        for i in range(0, len(full_token_list) - self.max_seq_len, self.max_seq_len):\n",
    "            all_ids.append(full_token_list[i : i + self.max_seq_len])\n",
    "        \n",
    "        return all_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence (x): first max_seq_len-1 tokens\n",
    "        # Target sequence (y): last max_seq_len-1 tokens (shifted by 1)\n",
    "        sequence = torch.tensor(self.all_ids[idx], dtype=torch.long)\n",
    "        \n",
    "        # The target sequence is the input sequence shifted by one token\n",
    "        # sequence[:-1] is the input (context)\n",
    "        # sequence[1:] is the target (next tokens)\n",
    "        return sequence[:-1], sequence[1:]\n",
    "\n",
    "def create_dataloaders(tokenizer, train_data, valid_data, test_data, max_seq_len, batch_size):\n",
    "    # Use max_seq_len + 1 tokens to get an input sequence of length max_seq_len\n",
    "    train_dataset = LanguageModelingDataset(train_data, tokenizer, max_seq_len + 1)\n",
    "    valid_dataset = LanguageModelingDataset(valid_data, tokenizer, max_seq_len + 1)\n",
    "    test_dataset = LanguageModelingDataset(test_data, tokenizer, max_seq_len + 1)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. LSTM Model ---\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, text, hidden_state=None):\n",
    "        # text shape: [batch size, seq len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: [batch size, seq len, emb dim]\n",
    "        \n",
    "        # Initial hidden state will be created if not passed\n",
    "        output, (hidden, cell) = self.lstm(embedded, hidden_state)\n",
    "        # output shape: [batch size, seq len, hidden dim]\n",
    "        \n",
    "        prediction = self.fc(self.dropout(output))\n",
    "        # prediction shape: [batch size, seq len, vocab size]\n",
    "        return prediction, (hidden, cell)\n",
    "\n",
    "# --- 2. Transformer Model (Decoder Only) ---\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_heads, n_layers, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
    "        nn.init.uniform_(self.pos_encoder, -0.01, 0.01)\n",
    "\n",
    "        # Transformer Decoder Layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=emb_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=4*emb_dim, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch size, seq len]\n",
    "        seq_len = src.shape[1]\n",
    "        \n",
    "        # 1. Embedding + Positional Encoding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        embedded = embedded + self.pos_encoder[:, :seq_len, :]\n",
    "        # embedded shape: [batch size, seq len, emb dim]\n",
    "\n",
    "        # 2. Causal Mask (Look-ahead mask)\n",
    "        # Allows tokens to only attend to previous tokens.\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(DEVICE)\n",
    "        \n",
    "        # 3. Transformer Decoder\n",
    "        output = self.transformer_decoder(\n",
    "            embedded, \n",
    "            tgt_mask=mask, \n",
    "            memory=embedded # Using self-attention for decoder-only LM\n",
    "        )\n",
    "        # output shape: [batch size, seq len, emb dim]\n",
    "        \n",
    "        # 4. Final Prediction\n",
    "        prediction = self.fc(self.dropout(output))\n",
    "        # prediction shape: [batch size, seq len, vocab size]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and chunking data (PAD ID: 1, Vocab Size: 10004)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() should return None, not 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mpad_id)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 2. Create DataLoaders\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m train_loader, valid_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Experiments for Tokenizer: **\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m** (Vocab: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[0;34m(tokenizer, train_data, valid_data, test_data, max_seq_len, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_dataloaders\u001b[39m(tokenizer, train_data, valid_data, test_data, max_seq_len, batch_size):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Use max_seq_len + 1 tokens to get an input sequence of length max_seq_len\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModelingDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     valid_dataset \u001b[38;5;241m=\u001b[39m LanguageModelingDataset(valid_data, tokenizer, max_seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m LanguageModelingDataset(test_data, tokenizer, max_seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() should return None, not 'int'"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(data_loader):\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            \n",
    "            if isinstance(model, LSTMLanguageModel):\n",
    "                # Pass None for initial hidden state\n",
    "                output, _ = model(src)\n",
    "            else: # TransformerLanguageModel\n",
    "                output = model(src)\n",
    "\n",
    "            # Reshape for loss calculation: \n",
    "            # Output: [batch size * seq len, vocab size]\n",
    "            # Target: [batch size * seq len]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item() * trg.numel()\n",
    "            total_tokens += trg.numel()\n",
    "\n",
    "    # NLL is the average cross-entropy loss\n",
    "    nll = total_loss / total_tokens\n",
    "    # PPL is the exponential of NLL\n",
    "    ppl = torch.exp(torch.tensor(nll)).item() \n",
    "    \n",
    "    return nll, ppl\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, criterion, num_epochs):\n",
    "    best_valid_ppl = float('inf')\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (src, trg) in enumerate(train_loader):\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if isinstance(model, LSTMLanguageModel):\n",
    "                # Detach hidden state for sequence modeling\n",
    "                output, _ = model(src, None)\n",
    "            else:\n",
    "                output = model(src)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        valid_nll, valid_ppl = evaluate(model, valid_loader, criterion)\n",
    "        \n",
    "        print(f\"  Epoch: {epoch}, Train Loss: {epoch_loss/len(train_loader):.4f}, Valid PPL: {valid_ppl:.2f}\")\n",
    "\n",
    "        if valid_ppl < best_valid_ppl:\n",
    "            best_valid_ppl = valid_ppl\n",
    "            # Save the best model state\n",
    "            # torch.save(model.state_dict(), 'best_model.pt') \n",
    "            \n",
    "    print(f\"Training finished. Best Valid PPL: {best_valid_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Full Experiment Runner ---\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizers['word'].word_to_id.get(\"<pad>\")) # Using WordTokenizer PAD for initial criterion, will update\n",
    "results = {}\n",
    "\n",
    "tokenizer_names = [\"word\", \"bpe\", \"byte\", \"unigram\"]\n",
    "model_names = [\"lstm\", \"transformer\"]\n",
    "\n",
    "for t_name in tokenizer_names:\n",
    "    tokenizer = tokenizers[t_name]\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    pad_id = tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else tokenizer._get_pad_id(tokenizer)\n",
    "\n",
    "    # 1. Update the criterion with the current tokenizer's PAD ID\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    \n",
    "    # 2. Create DataLoaders\n",
    "    train_loader, valid_loader, test_loader = create_dataloaders(\n",
    "        tokenizer, train_data, valid_data, test_data, MAX_SEQ_LEN, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"\\n========================================================\")\n",
    "    print(f\"Starting Experiments for Tokenizer: **{t_name.upper()}** (Vocab: {vocab_size})\")\n",
    "    print(f\"========================================================\")\n",
    "\n",
    "    for m_name in model_names:\n",
    "        print(f\"\\n--- Running **{m_name.upper()}** Model ---\")\n",
    "        \n",
    "        if m_name == \"lstm\":\n",
    "            model = LSTMLanguageModel(\n",
    "                vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT\n",
    "            ).to(DEVICE)\n",
    "        else: # transformer\n",
    "            model = TransformerLanguageModel(\n",
    "                vocab_size, EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, MAX_SEQ_LEN, DROPOUT\n",
    "            ).to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Train the model (Using a small number of epochs for demonstration)\n",
    "        train_model(model, train_loader, valid_loader, optimizer, criterion, NUM_EPOCHS)\n",
    "        \n",
    "        # Evaluate on the Test Set\n",
    "        test_nll, test_ppl = evaluate(model, test_loader, criterion)\n",
    "        results[(t_name, m_name)] = {\"NLL\": test_nll, \"PPL\": test_ppl}\n",
    "        \n",
    "        print(f\"\\nâœ… **{t_name.upper()}** + **{m_name.upper()}** TEST RESULTS:\")\n",
    "        print(f\"  Test NLL: {test_nll:.4f}\")\n",
    "        print(f\"  Test PPL: {test_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Final Results Table ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EXPERIMENT RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"| Tokenizer | Model | Test NLL | Test PPL |\")\n",
    "print(\"| :---: | :---: | :---: | :---: |\")\n",
    "for (t_name, m_name), metrics in results.items():\n",
    "    print(f\"| {t_name.capitalize()} | {m_name.capitalize()} | {metrics['NLL']:.4f} | {metrics['PPL']:.2f} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l A100)",
   "language": "python",
   "name": "d2l-a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
