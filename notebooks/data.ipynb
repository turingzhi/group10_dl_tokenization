{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Make sure src/ is visible\n",
    "project_root = os.path.abspath(\".\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.config import config\n",
    "from src.training.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(overrides=None):\n",
    "    cfg = config.copy()\n",
    "    if overrides:\n",
    "        cfg.update(overrides)\n",
    "    return cfg\n",
    "\n",
    "def show_config(cfg):\n",
    "    for k, v in cfg.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(tokenizer_name: str, model_type: str, results_dir: str = \"results\"):\n",
    "    \"\"\"\n",
    "    Load training metrics from JSON and plot training vs validation loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer_name : str\n",
    "        The tokenizer identifier (e.g., \"word\", \"bpe\", \"unigram\", etc.).\n",
    "    model_type : str\n",
    "        The model type (e.g., \"transformer\", \"lstm\").\n",
    "    results_dir : str, default=\"results\"\n",
    "        Directory where metrics JSON files are stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct file path\n",
    "    metrics_path = f\"{results_dir}/metrics_{tokenizer_name}_{model_type}.json\"\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    # Extract values\n",
    "    epochs = [m[\"epoch\"] for m in metrics]\n",
    "    train_losses = [m[\"train_loss\"] for m in metrics]\n",
    "    val_losses = [m[\"val_loss\"] for m in metrics]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, train_losses, marker='o', label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, marker='o', label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training vs Validation Loss ({tokenizer_name}-{model_type})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(tokenizer_name, model_type, cfg):\n",
    "    print(\"Starting training with config:\")\n",
    "    show_config(cfg)\n",
    "\n",
    "    # Your train_model already prints metrics and saves files.\n",
    "    train_model(\n",
    "        tokenizer_name=tokenizer_name,\n",
    "        model_type=model_type,\n",
    "        cfg_overrides=cfg\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"word\"          # or bpe, unigram, byte\n",
    "model_type = \"transformer\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"word\"          # or bpe, unigram, byte\n",
    "model_type = \"lstm\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bpe transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"bpe\"          # or bpe, unigram, byte\n",
    "model_type = \"transformer\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 8e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bpe lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"bpe\"          # or bpe, unigram, byte\n",
    "model_type = \"lstm\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"unigram\"          # or bpe, unigram, byte\n",
    "model_type = \"transformer\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"unigram\"          # or bpe, unigram, byte\n",
    "model_type = \"lstm\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "byte transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"byte\"          # or bpe, unigram, byte\n",
    "model_type = \"transformer\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "byte lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"byte\"          # or bpe, unigram, byte\n",
    "model_type = \"lstm\"       # or lstm\n",
    "\n",
    "cfg_overrides = {\n",
    "     \"dataset_name\": \"wikitext-2-raw-v1\",\n",
    "    \"context_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "cfg = build_config(cfg_overrides)\n",
    "show_config(cfg)\n",
    "run_training(tokenizer_name, model_type, cfg)\n",
    "plot_metrics(\"word\", \"transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l-GPU)",
   "language": "python",
   "name": "d2l-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
